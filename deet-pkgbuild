#!/usr/bin/env python
# -*- coding: utf-8 -*-

from bs4 import BeautifulSoup

import requests
import sys


def scrape_urls(pkg):
    repos = ('packages', 'community')
    base = 'https://git.archlinux.org/svntogit/{}.git/tree/{}/trunk'
    for repo in repos:
        req = requests.get(base.format(repo, pkg))
        if req.ok:
            break
    else:
        print('Error requesting :: <{}>'.format(pkg))
        sys.exit(0)
    soup = BeautifulSoup(req.text, "html.parser")
    a_tags = soup.findAll('a')
    hrefs = {a_tags[i]['href'] for i in range(len(a_tags))}
    hrefs = {ref for ref in hrefs if '/tree/{}/trunk/'.format(pkg) in ref}
    base = 'https://git.archlinux.org{}'
    for url in hrefs:
        req = requests.get(base.format(url.replace('tree', 'plain')))
        filename = url.split('/')[-1]
        with open(filename, 'w+', encoding='utf-8') as fh:
            fh.write(req.text)

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print('Pass a package-name')
        sys.exit(0)
    pkg = sys.argv[1]
    pkg_urls = scrape_urls(pkg)
